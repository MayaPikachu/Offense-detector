{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjtDNDqhnanzQ9kqIwUJ0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MayaPikachu/Offense-detector/blob/main/Offense_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install unidecode\n",
        "!pip install tokenizers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ec0PCg0fGFdF",
        "outputId": "03198dc9-032b-4de1-cb55-e32ddf7b9dbd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/uc?id=18VOWo-rHUiQ7pIlzzMzkFCrZNX0RIWCe\"\n",
        "!gdown --fuzzy \"https://drive.google.com/uc?id=1G-bAySkxcI1laV9wDkWGF1mxENICzglN\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRlLTmf2cLyT",
        "outputId": "2e606d88-4a23-4f0d-da47-8088f2475151"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18VOWo-rHUiQ7pIlzzMzkFCrZNX0RIWCe\n",
            "To: /content/SBIC.v2.trn.csv\n",
            "100% 27.7M/27.7M [00:00<00:00, 83.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G-bAySkxcI1laV9wDkWGF1mxENICzglN\n",
            "To: /content/SBIC.v2.tst.csv\n",
            "100% 4.40M/4.40M [00:00<00:00, 29.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OBvYYFfRzcRc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from unidecode import unidecode\n",
        "from tokenizers import ByteLevelBPETokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"SBIC.v2.trn.csv\")\n",
        "dt = pd.read_csv(\"SBIC.v2.tst.csv\")"
      ],
      "metadata": {
        "id": "MeVH-BIfzy74"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poststrn = df.post.values\n",
        "resultstrn = df.offensiveYN.values\n",
        "\n",
        "poststst = dt.post.values\n",
        "resultstst = dt.offensiveYN.values\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train_from_iterator([poststrn[:int(4E7)]], vocab_size=500, min_frequency=5)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "print(f\"\\n\\n\\tTokenizado! Vocab_size = {vocab_size}\\n\")\n",
        "\n",
        "encode = lambda s: tokenizer.encode(s).ids\n",
        "decode = lambda ids: tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDcvK7u80Fgq",
        "outputId": "1050c9a2-54fe-4bea-fbea-ca81ba26d2e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\tTokenizado! Vocab_size = 500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 30\n",
        "batch_size = 64\n",
        "iter = 3000\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def get_data(file, results, batch_size = batch_size):\n",
        "  X, Y = [], []\n",
        "  for i in range(batch_size):\n",
        "    ix = torch.randint(0, len(file), (1, ))\n",
        "    input = (encode(file[ix]) + [0] * block_size)[:block_size]\n",
        "    X.append(input)\n",
        "    Y.append(results[ix])\n",
        "  return torch.tensor(X).float(), torch.nan_to_num(torch.tensor(Y).float(), nan=0.0)\n",
        "\n",
        "get_data(poststrn, resultstrn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGzEJWlZHLrz",
        "outputId": "8f94c204-bb17-4d3d-8168-63eb87e41383"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[379., 326., 280.,  ..., 277., 271., 320.],\n",
              "         [ 43.,  64.,  69.,  ..., 342.,  82., 311.],\n",
              "         [ 41., 373., 394.,  ...,  85., 283., 285.],\n",
              "         ...,\n",
              "         [ 31.,  54., 280.,  ...,   0.,   0.,   0.],\n",
              "         [ 49.,  64.,  66.,  ...,   0.,   0.,   0.],\n",
              "         [ 51., 258., 262.,  ..., 266.,  82., 256.]]),\n",
              " tensor([1.0000, 0.0000, 0.5000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.5000, 1.0000, 0.0000,\n",
              "         1.0000, 0.5000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
              "         1.0000, 1.0000, 0.5000, 0.0000, 0.0000, 1.0000, 0.5000, 0.0000, 0.0000,\n",
              "         1.0000, 0.0000, 1.0000, 0.5000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
              "         1.0000, 0.0000, 0.0000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, n_hidden):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(block_size, n_hidden)\n",
        "    self.l2 = nn.Linear(n_hidden, n_hidden)\n",
        "    self.l3 = nn.Linear(n_hidden, 1)\n",
        "    self.s = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.l1(x))\n",
        "    x = nn.Dropout(0.2)(F.relu(self.l2(x)))\n",
        "    x = nn.Dropout(0.2)(self.l3(x))\n",
        "    return self.s(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "fwRz8T8DNWdz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(128)\n",
        "model.to(device)\n",
        "params = list(model.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.0003)\n",
        "loss_fn = nn.BCELoss()\n",
        "for i in range(iter):\n",
        "  X, Y = get_data(poststrn, resultstrn)\n",
        "  X = X.to(device)\n",
        "  Y = Y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logits = model(X)\n",
        "  loss = loss_fn(logits, Y.unsqueeze(1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if (i%100 == 0):\n",
        "    print(f'step = {i}, loss = {loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HWjGb4FZ4-y",
        "outputId": "7682fc35-5f82-4e6a-f4bc-436d4818faef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 0, loss = 41.52538299560547\n",
            "step = 100, loss = 2.025615930557251\n",
            "step = 200, loss = 1.2374787330627441\n",
            "step = 300, loss = 1.0146201848983765\n",
            "step = 400, loss = 0.6715511083602905\n",
            "step = 500, loss = 0.6472303867340088\n",
            "step = 600, loss = 0.6880075931549072\n",
            "step = 700, loss = 0.7195512056350708\n",
            "step = 800, loss = 0.6726694107055664\n",
            "step = 900, loss = 0.7258880138397217\n",
            "step = 1000, loss = 0.657617449760437\n",
            "step = 1100, loss = 0.7024713158607483\n",
            "step = 1200, loss = 0.7306045293807983\n",
            "step = 1300, loss = 0.6448284983634949\n",
            "step = 1400, loss = 0.6326622366905212\n",
            "step = 1500, loss = 0.6957944631576538\n",
            "step = 1600, loss = 0.7472766041755676\n",
            "step = 1700, loss = 0.6569730043411255\n",
            "step = 1800, loss = 0.6665205955505371\n",
            "step = 1900, loss = 0.674483060836792\n",
            "step = 2000, loss = 0.6588184237480164\n",
            "step = 2100, loss = 0.66803377866745\n",
            "step = 2200, loss = 0.7171688675880432\n",
            "step = 2300, loss = 0.6251607537269592\n",
            "step = 2400, loss = 0.6151273250579834\n",
            "step = 2500, loss = 0.6658488512039185\n",
            "step = 2600, loss = 0.6772423982620239\n",
            "step = 2700, loss = 0.7037951946258545\n",
            "step = 2800, loss = 0.6891937255859375\n",
            "step = 2900, loss = 0.6460229754447937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precisao = 0.0\n",
        "for i in range(30):\n",
        "  X, Y = get_data(poststst, resultstst)\n",
        "  X = X.to(device)\n",
        "  Y = Y.to(device)\n",
        "  logits = model(X)\n",
        "  precisao += (logits.round() == Y.unsqueeze(1)).float().mean()/30.\n",
        "  print(f'x: {decode(X[1].int().to(\"cpu\").tolist())}, y = {Y[1].to(\"cpu\")}, predito = {0 if logits[1].to(\"cpu\").item() < .5 else 1}')\n",
        "\n",
        "print(precisao)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbOLg0ewVNUJ",
        "outputId": "a4185344-6f2c-428e-d9ce-624c29b7580f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: What do you call a Jewish helicopter? A holocaupter!!!!!!, y = 1.0, predito = 1\n",
            "x: What’s the the difference between Al Qaeda base and a Pakist, y = 1.0, predito = 1\n",
            "x: I will never in my life step foot into an Arab state other than, y = 1.0, predito = 1\n",
            "x: Why does Japan have a low obesity rate and a low birth rate, y = 1.0, predito = 1\n",
            "x: What do you call a Jew with autism? Auschwistic!!!!!!!!!!, y = 1.0, predito = 1\n",
            "x: The fucking #fakenews @ABC, @NBCNews, @CBSNews, y = 1.0, predito = 0\n",
            "x: 15 minutes. Nervous. Not because of speaking, but because of the, y = 0.0, predito = 0\n",
            "x: Yeah, child labor IS illegal. Unless you mean \"w, y = 1.0, predito = 1\n",
            "x: whats the difference between a black person and a jew? Depends on the, y = 1.0, predito = 1\n",
            "x: How do you pick up black girls? You burn their village, chain, y = 1.0, predito = 0\n",
            "x: RT @Smith_Wessern: Some hoes live jus to hit the clubs, y = 1.0, predito = 0\n",
            "x: How you gonna call my ex worthless when you fucked him too what the fuck!!, y = 1.0, predito = 0\n",
            "x: a jew and a black are about to get shot, who dies first the jew had coins as, y = 1.0, predito = 1\n",
            "x: Maria makes a good point...!!!!!!!!!!!!!!!, y = 0.0, predito = 1\n",
            "x: (Stolen) A black woman calls her 5 kids Tyrone. How can she t, y = 1.0, predito = 0\n",
            "x: I like to see the cope skills in the comments of PUA videos\n",
            "t, y = 0.5, predito = 1\n",
            "x: Now this is something. I want onew n minho to have dogs n all their do, y = 0.0, predito = 0\n",
            "x: What according to you should be at the top of priorities for policyma, y = 0.0, predito = 1\n",
            "x: If people from the 1800s could see us today, I think they'd be l, y = 0.0, predito = 0\n",
            "x: My sense of humour is so dark It pays child support!!!, y = 1.0, predito = 0\n",
            "x: What do you call a white guy surrounded by a thousand black guys, y = 1.0, predito = 1\n",
            "x: I WANT FUCKING FREEDOMMMMMMM I JUST, y = 1.0, predito = 1\n",
            "x: My lil cousin be hating. Bitches always want the older nig, y = 1.0, predito = 0\n",
            "x: I cannot be worried 'bout no bitch\n",
            "Cause I know a bitch could be some, y = 1.0, predito = 1\n",
            "x: NEET incels how you spend time and cope with severe depress, y = 0.0, predito = 0\n",
            "x: All I've been thinking about lately when William deploys for a ye, y = 0.0, predito = 0\n",
            "x: I bought a girl at an auction She had no teeth!!!!!, y = 1.0, predito = 1\n",
            "x: Surveying gender is easy because there's only male and female.!, y = 1.0, predito = 1\n",
            "x: Lets ask them to stop by the Federal Reserve and grab those book, y = 0.0, predito = 1\n",
            "x: I'm about to see my best friend for the first time since like Novemb, y = 0.0, predito = 1\n",
            "tensor(0.5370)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(tokenizer.get_vocab(), key=lambda x: tokenizer.get_vocab()[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfbcnBCVv-Da",
        "outputId": "00b8669a-3b83-453c-d1ab-007fcbcb93f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~',\n",
              " '¡',\n",
              " '¢',\n",
              " '£',\n",
              " '¤',\n",
              " '¥',\n",
              " '¦',\n",
              " '§',\n",
              " '¨',\n",
              " '©',\n",
              " 'ª',\n",
              " '«',\n",
              " '¬',\n",
              " '®',\n",
              " '¯',\n",
              " '°',\n",
              " '±',\n",
              " '²',\n",
              " '³',\n",
              " '´',\n",
              " 'µ',\n",
              " '¶',\n",
              " '·',\n",
              " '¸',\n",
              " '¹',\n",
              " 'º',\n",
              " '»',\n",
              " '¼',\n",
              " '½',\n",
              " '¾',\n",
              " '¿',\n",
              " 'À',\n",
              " 'Á',\n",
              " 'Â',\n",
              " 'Ã',\n",
              " 'Ä',\n",
              " 'Å',\n",
              " 'Æ',\n",
              " 'Ç',\n",
              " 'È',\n",
              " 'É',\n",
              " 'Ê',\n",
              " 'Ë',\n",
              " 'Ì',\n",
              " 'Í',\n",
              " 'Î',\n",
              " 'Ï',\n",
              " 'Ð',\n",
              " 'Ñ',\n",
              " 'Ò',\n",
              " 'Ó',\n",
              " 'Ô',\n",
              " 'Õ',\n",
              " 'Ö',\n",
              " '×',\n",
              " 'Ø',\n",
              " 'Ù',\n",
              " 'Ú',\n",
              " 'Û',\n",
              " 'Ü',\n",
              " 'Ý',\n",
              " 'Þ',\n",
              " 'ß',\n",
              " 'à',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ã',\n",
              " 'ä',\n",
              " 'å',\n",
              " 'æ',\n",
              " 'ç',\n",
              " 'è',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'ë',\n",
              " 'ì',\n",
              " 'í',\n",
              " 'î',\n",
              " 'ï',\n",
              " 'ð',\n",
              " 'ñ',\n",
              " 'ò',\n",
              " 'ó',\n",
              " 'ô',\n",
              " 'õ',\n",
              " 'ö',\n",
              " '÷',\n",
              " 'ø',\n",
              " 'ù',\n",
              " 'ú',\n",
              " 'û',\n",
              " 'ü',\n",
              " 'ý',\n",
              " 'þ',\n",
              " 'ÿ',\n",
              " 'Ā',\n",
              " 'ā',\n",
              " 'Ă',\n",
              " 'ă',\n",
              " 'Ą',\n",
              " 'ą',\n",
              " 'Ć',\n",
              " 'ć',\n",
              " 'Ĉ',\n",
              " 'ĉ',\n",
              " 'Ċ',\n",
              " 'ċ',\n",
              " 'Č',\n",
              " 'č',\n",
              " 'Ď',\n",
              " 'ď',\n",
              " 'Đ',\n",
              " 'đ',\n",
              " 'Ē',\n",
              " 'ē',\n",
              " 'Ĕ',\n",
              " 'ĕ',\n",
              " 'Ė',\n",
              " 'ė',\n",
              " 'Ę',\n",
              " 'ę',\n",
              " 'Ě',\n",
              " 'ě',\n",
              " 'Ĝ',\n",
              " 'ĝ',\n",
              " 'Ğ',\n",
              " 'ğ',\n",
              " 'Ġ',\n",
              " 'ġ',\n",
              " 'Ģ',\n",
              " 'ģ',\n",
              " 'Ĥ',\n",
              " 'ĥ',\n",
              " 'Ħ',\n",
              " 'ħ',\n",
              " 'Ĩ',\n",
              " 'ĩ',\n",
              " 'Ī',\n",
              " 'ī',\n",
              " 'Ĭ',\n",
              " 'ĭ',\n",
              " 'Į',\n",
              " 'į',\n",
              " 'İ',\n",
              " 'ı',\n",
              " 'Ĳ',\n",
              " 'ĳ',\n",
              " 'Ĵ',\n",
              " 'ĵ',\n",
              " 'Ķ',\n",
              " 'ķ',\n",
              " 'ĸ',\n",
              " 'Ĺ',\n",
              " 'ĺ',\n",
              " 'Ļ',\n",
              " 'ļ',\n",
              " 'Ľ',\n",
              " 'ľ',\n",
              " 'Ŀ',\n",
              " 'ŀ',\n",
              " 'Ł',\n",
              " 'ł',\n",
              " 'Ń',\n",
              " 'Ġt',\n",
              " 'Ġa',\n",
              " 'he',\n",
              " 'in',\n",
              " 'Ġs',\n",
              " 'Ġw',\n",
              " 're',\n",
              " 'ou',\n",
              " 'Ġb',\n",
              " 'Ġthe',\n",
              " 'on',\n",
              " 'ha',\n",
              " 'er',\n",
              " 'Ġm',\n",
              " 'it',\n",
              " 'ing',\n",
              " 'Ġf',\n",
              " 'is',\n",
              " 'Ġc',\n",
              " 'Ġd',\n",
              " 'nd',\n",
              " 'll',\n",
              " 'or',\n",
              " 'an',\n",
              " 'es',\n",
              " 'Ġto',\n",
              " 'Ġp',\n",
              " 'en',\n",
              " 'Ġg',\n",
              " 'Ġl',\n",
              " 'Ġy',\n",
              " 'Ġo',\n",
              " 'ed',\n",
              " 'hat',\n",
              " 'Ġn',\n",
              " 'ĠI',\n",
              " 'om',\n",
              " 'Ġyou',\n",
              " 'ck',\n",
              " 'at',\n",
              " 'Ġand',\n",
              " 'ar',\n",
              " 'Ġin',\n",
              " 'st',\n",
              " 'et',\n",
              " 'ot',\n",
              " 'Ġof',\n",
              " 'le',\n",
              " 've',\n",
              " 'ow',\n",
              " 'Ġh',\n",
              " 'Ġha',\n",
              " 'ay',\n",
              " 'al',\n",
              " '..',\n",
              " 'as',\n",
              " 'Ġbe',\n",
              " 'ic',\n",
              " 'ke',\n",
              " 'Ġis',\n",
              " 'id',\n",
              " 'Ġon',\n",
              " 'Ġe',\n",
              " 'se',\n",
              " 'Ġth',\n",
              " 'Ġhe',\n",
              " 'ig',\n",
              " 'Ġu',\n",
              " 'ld',\n",
              " 'Ġit',\n",
              " 'Ġdo',\n",
              " 'ut',\n",
              " 'ir',\n",
              " 'ĠT',\n",
              " 'am',\n",
              " 'ver',\n",
              " 'ad',\n",
              " 'Ġthat',\n",
              " 'oo',\n",
              " 'Ġre',\n",
              " 'all',\n",
              " 'im',\n",
              " 'op',\n",
              " 'Ġfor',\n",
              " 'Ġli',\n",
              " 'Ġmy',\n",
              " 'ĠA',\n",
              " \"'t\",\n",
              " 'Ġst',\n",
              " 'out',\n",
              " 'ce',\n",
              " \"'s\",\n",
              " 'la',\n",
              " 'ch',\n",
              " 'ly',\n",
              " 'Ġr',\n",
              " 'ion',\n",
              " 'Ġme',\n",
              " 'Ġj',\n",
              " 'Ġ.',\n",
              " 'ĠS',\n",
              " 'Ġi',\n",
              " 'ith',\n",
              " '&#',\n",
              " 'Ġwh',\n",
              " 'Ġan',\n",
              " '12',\n",
              " 'ent',\n",
              " 'Ġare',\n",
              " 'Ġk',\n",
              " 'Ġwith',\n",
              " 'ill',\n",
              " 'uck',\n",
              " 'ĠM',\n",
              " 'Ġhave',\n",
              " 'ri',\n",
              " 'Ġwe',\n",
              " 'ust',\n",
              " 'ht',\n",
              " 'ĠB',\n",
              " 'Ġlike',\n",
              " 'Ġse',\n",
              " 'âĢ',\n",
              " 'What',\n",
              " 'ct',\n",
              " 'Ġthey',\n",
              " 'Ġas',\n",
              " 'Ġ@',\n",
              " 'ist',\n",
              " 'Ġ\"',\n",
              " 'Ġsh',\n",
              " 'ĠC',\n",
              " '128',\n",
              " 'Ġne',\n",
              " 'ome',\n",
              " 'Ġwas',\n",
              " 'ĠW',\n",
              " 'lack',\n",
              " 'Ġfuck',\n",
              " 'Ġso',\n",
              " 'ould',\n",
              " 'ra',\n",
              " 'ro',\n",
              " 'her',\n",
              " 'fe',\n",
              " 'hen',\n",
              " 'us',\n",
              " 'Ġcan',\n",
              " 'ers',\n",
              " 'ĠH',\n",
              " 'um',\n",
              " 'ur',\n",
              " 'Ġget',\n",
              " 'Ġ#',\n",
              " 'Ġnot',\n",
              " 'Ġab',\n",
              " 'au',\n",
              " 'Ġ,',\n",
              " 'Ġat',\n",
              " 'one',\n",
              " 'ight',\n",
              " 'un',\n",
              " 'Ġblack',\n",
              " 'Ġbut',\n",
              " 'ol',\n",
              " 'ant',\n",
              " 'em',\n",
              " 'Ġthis',\n",
              " '...',\n",
              " 'ter',\n",
              " 'eop',\n",
              " 'Ġyour',\n",
              " 'eople',\n",
              " 'pe',\n",
              " 'il',\n",
              " 'Ġall',\n",
              " 'ĠN',\n",
              " 'ore',\n",
              " '1285',\n",
              " 'Ġup',\n",
              " 'ge',\n",
              " 'Ġgo',\n",
              " 'ĠD',\n",
              " 'ĠF',\n",
              " 'ally',\n",
              " 'Ġjust',\n",
              " 'ite',\n",
              " 'ĠP',\n",
              " 'ren',\n",
              " 'Ġpeople',\n",
              " 'est',\n",
              " 'Ġdi',\n",
              " 'el',\n",
              " 'pp',\n",
              " 'ew',\n",
              " 'th',\n",
              " 'Ġwor',\n",
              " 'Ġfr',\n",
              " 'Ġabout',\n",
              " ';&#',\n",
              " 'Ġdon',\n",
              " 'Ġwhen',\n",
              " 'Ġout',\n",
              " 'Ġma',\n",
              " 'ry',\n",
              " 'ĠThe',\n",
              " 'Ġkn',\n",
              " \"Ġ'\",\n",
              " 'king',\n",
              " 'Ġ?',\n",
              " 'Ġor',\n",
              " 'han',\n",
              " \"'m\",\n",
              " 'ess',\n",
              " 'ĠG',\n",
              " 'art',\n",
              " 'ause',\n",
              " 'Ġsu',\n",
              " 'Ġwom',\n",
              " 'ĠJ',\n",
              " 'our',\n",
              " 'Ġcom',\n",
              " 'âĢĻ',\n",
              " 'ab',\n",
              " 'ast',\n",
              " 'Ġfucking',\n",
              " 'very',\n",
              " 'Ġv',\n",
              " 'ation',\n",
              " 'Ġman',\n",
              " 'Ġal',\n",
              " 'we',\n",
              " 'ood',\n",
              " 'ĠR',\n",
              " 'Ġfrom',\n",
              " 'Ġknow',\n",
              " 'ind',\n",
              " 'hy',\n",
              " 'ĠL',\n",
              " 'Ġde',\n",
              " 'Ġif',\n",
              " 'Ġher',\n",
              " '\"\"',\n",
              " 'Ġthem']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}